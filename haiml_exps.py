# -*- coding: utf-8 -*-
"""HAIML EXPS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ivq4XI9hRnnNmIu4NV_ixeVsc1xU0bRN

EXP 1
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

file_path = 'diabetes.csv'
data = pd.read_csv(file_path)

# Display basic information about the dataset
print("Initial Dataset Info:")
print(data.info())

# Check for missing values
print("\nMissing Values in Dataset:")
print(data.isnull().sum())

# Drop duplicates if any
data_cleaned = data.drop_duplicates()

# Identify columns that may need transformation (e.g., scaling or encoding categorical data)
print("\nColumns in Dataset:")
print(data_cleaned.columns)

# Display first few rows to inspect the data
print("\nSample of the Dataset:")
print(data_cleaned.head())

# Basic statistical summary of the data
print("\nStatistical Summary:")
print(data_cleaned.describe())

from sklearn.preprocessing import StandardScaler
numeric_cols = data_cleaned.select_dtypes(include=[np.number]).columns
scaler = StandardScaler()
data_cleaned[numeric_cols] = scaler.fit_transform(data_cleaned[numeric_cols])
# Show transformed data (first few rows)
print("\nTransformed Dataset:")
print(data_cleaned.head())

for col in numeric_cols:
    Q1 = data_cleaned[col].quantile(0.25)
    Q3 = data_cleaned[col].quantile(0.75)
    IQR = Q3 - Q1
    # Define outliers as data points outside 1.5 * IQR range
    filter = ~((data_cleaned[col] < (Q1 - 1.5 * IQR)) | (data_cleaned[col] > (Q3 + 1.5 * IQR)))
    # Keep only the data points that are not outliers
    data_cleaned = data_cleaned[filter]

data_cleaned

# Save the cleaned and transformed dataset to a new CSV
cleaned_file_path = 'diabetes_cleaned.csv'
data_cleaned.to_csv(cleaned_file_path, index=False)

# Set the aesthetics for seaborn
sns.set(style="whitegrid")

# 1. Histograms for each numeric feature to show distributions
def plot_histograms(data):
    data.hist(figsize=(12, 10), bins=20, edgecolor='black')
    plt.suptitle("Histograms of All Features", fontsize=16)
    plt.show()

# 2. Correlation Heatmap
def plot_correlation_heatmap(data):
    plt.figure(figsize=(10, 8))
    corr_matrix = data.corr()
    sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", linewidths=0.5, fmt=".2f")
    plt.title("Correlation Heatmap", fontsize=16)
    plt.show()

# 3. Boxplots to visualize outliers
def plot_boxplots(data):
    plt.figure(figsize=(12, 8))
    sns.boxplot(data=data, orient='h', palette="Set2")
    plt.title("Boxplots of All Features", fontsize=16)
    plt.show()

# 4. Pair plot to show feature relationships
def plot_pairplot(data):
    sns.pairplot(data, hue="Outcome", diag_kind="kde", palette="husl", corner=True)
    plt.suptitle("Pair Plot of Features", fontsize=16)
    plt.show()

# 5. Bar plot for Outcome distribution
def plot_outcome_distribution(data):
    plt.figure(figsize=(6, 4))
    sns.countplot(x='Outcome', data=data, palette="coolwarm")
    plt.title("Distribution of Outcome Variable", fontsize=16)
    plt.show()

# Example: Run the visualizations on cleaned and transformed data
plot_histograms(data_cleaned)
plot_correlation_heatmap(data_cleaned)
plot_boxplots(data_cleaned)
plot_pairplot(data_cleaned)
plot_outcome_distribution(data_cleaned)

"""EXP 2"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
file_path = 'EDA-HEALTHCARE.csv'  # Replace with your file path
data = pd.read_csv(file_path)

# 1. Basic Data Exploration
print("First 5 rows of the dataset:")
print(data.head())

print("\nDataset Information:")
print(data.info())

print("\nStatistical Summary:")
print(data.describe())

# Checking for missing values
print("\nMissing Values in each column:")
print(data.isnull().sum())

data = data.dropna()
# Reset index after removing missing rows (optional)
data = data.reset_index(drop=True)
print(data.isnull().sum())

# 2. Univariate Non-Graphical Analysis
# Measures of central tendency (mean, median) and spread (variance, std)
for column in data.select_dtypes(include=[np.number]).columns:
    print(f"\nColumn: {column}")
    print(f"Mean: {data[column].mean()}")
    print(f"Median: {data[column].median()}")
    print(f"Variance: {data[column].var()}")
    print(f"Standard Deviation: {data[column].std()}")

# 3. Univariate Graphical Analysis
# Histograms
for column in data.select_dtypes(include=[np.number]).columns:
    plt.figure(figsize=(8, 4))
    sns.histplot(data[column], kde=True)
    plt.title(f"Histogram of {column}")
    plt.show()

# Boxplots
for column in data.select_dtypes(include=[np.number]).columns:
    plt.figure(figsize=(8, 4))
    sns.boxplot(x=data[column])
    plt.title(f"Boxplot of {column}")
    plt.show()

# 4. Multivariate Non-Graphical Analysis
# Correlation matrix - we need to filter out non-numeric columns
numeric_data = data.select_dtypes(include=[np.number])

# Check if there are any numeric columns before calculating correlation
if not numeric_data.empty:
    print("\nCorrelation Matrix:")
    correlation_matrix = numeric_data.corr()
    print(correlation_matrix)

# 5. Multivariate Graphical Analysis
# Heatmap for correlation matrix
plt.figure(figsize=(10, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title("Heatmap of Correlation Matrix")
plt.show()

# Scatterplots for each pair of numerical variables
sns.pairplot(data.select_dtypes(include=[np.number]))
plt.show()

"""EXP 3"""

!pip install spacy
!python -m spacy download en_core_web_md

import spacy
import pandas as pd

# Load the dataset
file_path = 'final_cbc_diagnoses_dataset_with_labels.csv'
df = pd.read_csv(file_path)

nlp = spacy.load('en_core_web_md')

# Define a function to perform entity extraction
def extract_entities(text):
    doc = nlp(text)
    entities = [(ent.text, ent.label_) for ent in doc.ents]
    return entities

# Apply the entity extraction on the 'long_title' column
df['entities'] = df['short_title'].apply(lambda x: extract_entities(str(x)))

# Show some examples of extracted entities
df[['short_title', 'entities']].head()

"""EXP 4"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score

# Load dataset
df = pd.read_csv('diabetes.csv')

df.head()

# Data Preprocessing: Replace zero values in relevant columns with the column mean
df[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']] = df[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].replace(0, df[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].mean())

# Check for missing values
print("\nMissing Values in Dataset:")
print(df.isnull().sum())

# Drop duplicates if any
df = df.drop_duplicates()

# Splitting the dataset into features and target
X = df.drop('Outcome', axis=1)  # Features
y = df['Outcome']  # Target

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model training
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

y_test

y_pred = model.predict(X_test)
y_pred

# Model evaluation
print(classification_report(y_test, y_pred))
print("Accuracy: ", accuracy_score(y_test, y_pred))
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)

# Display the confusion matrix
print("Confusion Matrix:\n", cm)

plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""EXP 5"""

# Install necessary libraries
!pip install lime scikit-learn

# Import necessary libraries
import numpy as np
import pandas as pd
import shap
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from lime.lime_text import LimeTextExplainer
from sklearn.pipeline import make_pipeline

# Sample data: Sentences and corresponding sentiment labels (1: Positive, 0: Negative)
data = {
    "text": [
        "The doctor was very helpful and attentive during my visit.",
        "I had a terrible experience with the hospital staff.",
        "The treatment was effective and I feel much better now.",
        "The waiting time was too long, and the nurses were rude.",
        "Great service! The team was very professional.",
        "I wouldn't recommend this clinic due to poor service."
    ],
    "sentiment": [1, 0, 1, 0, 1, 0]  # 1: Positive, 0: Negative
}

# Convert data into a DataFrame
df = pd.DataFrame(data)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size=0.2, random_state=42)

# Create a TF-IDF vectorizer and Logistic Regression classifier pipeline
vectorizer = TfidfVectorizer()
classifier = LogisticRegression()

# Combine vectorizer and classifier in a pipeline
model_pipeline = make_pipeline(vectorizer, classifier)

# Train the model
model_pipeline.fit(X_train, y_train)

# 1. Local Interpretability using LIME
explainer_lime = LimeTextExplainer(class_names=["Negative", "Positive"])

# Predict the sentiment for a sample text and explain using LIME
sample_text = X_test.iloc[0]
exp = explainer_lime.explain_instance(sample_text, model_pipeline.predict_proba, num_features=5)

# Display the explanation
print(f"Prediction: {model_pipeline.predict([sample_text])[0]}")
print("Explanation:")
exp.show_in_notebook(text=sample_text)

# 2. Global Interpretability using SHAP
# Fit SHAP explainer (global interpretability)
explainer_shap = shap.Explainer(model_pipeline.named_steps['logisticregression'], vectorizer.transform(X_train))
shap_values = explainer_shap(vectorizer.transform(X_train))

# Plot the global feature importance using SHAP
shap.summary_plot(shap_values, vectorizer.transform(X_train), feature_names=vectorizer.get_feature_names_out())

#3. Counterfactual Explanation
def generate_counterfactual(text, word_to_change, new_word):
    """Modify the text by replacing a word and predict the new sentiment."""
    modified_text = text.replace(word_to_change, new_word)
    new_prediction = model_pipeline.predict([modified_text])[0]
    new_prob = model_pipeline.predict_proba([modified_text])[0]
    return modified_text, new_prediction, new_prob

# Example: Generate a counterfactual by changing "terrible" to "great"
cf_text, cf_prediction, cf_prob = generate_counterfactual(sample_text, "terrible", "great")

print(f"\nOriginal Sentence: '{sample_text}'")
print(f"Counterfactual Sentence: '{cf_text}'")
print(f"New Prediction: {cf_prediction} with probabilities {cf_prob}")

# 4. SHAP Visualization for the counterfactual sentence
cf_shap_values = explainer_shap(vectorizer.transform([cf_text]))
# SHAP Waterfall plot for counterfactual explanation
shap.waterfall_plot(cf_shap_values[0])
